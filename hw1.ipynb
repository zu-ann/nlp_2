{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from pymystem3 import Mystem\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from itertools import chain, combinations\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# скачаем данные в папке data и распакуем их\n",
    "PATH_TO_DATA = './data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [os.path.join(PATH_TO_DATA, file) for file in os.listdir(PATH_TO_DATA)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([pd.read_json(file, lines=True) for file in files], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1987, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(true_kws, predicted_kws):\n",
    "    assert len(true_kws) == len(predicted_kws)\n",
    "    \n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    jaccards = []\n",
    "    \n",
    "    for i in range(len(true_kws)):\n",
    "        true_kw = set(true_kws[i])\n",
    "        predicted_kw = set(predicted_kws[i])\n",
    "        \n",
    "        tp = len(true_kw & predicted_kw)\n",
    "        union = len(true_kw | predicted_kw)\n",
    "        fp = len(predicted_kw - true_kw)\n",
    "        fn = len(true_kw - predicted_kw)\n",
    "        \n",
    "        if (tp + fp) == 0:\n",
    "            prec = 0\n",
    "        else:\n",
    "            prec = tp / (tp + fp)\n",
    "        \n",
    "        if (tp + fn) == 0:\n",
    "            rec = 0\n",
    "        else:\n",
    "            rec = tp / (tp + fn)\n",
    "        if (prec + rec) == 0:\n",
    "            f1 = 0\n",
    "        else:\n",
    "            f1 = (2 * (prec * rec)) / (prec + rec)\n",
    "            \n",
    "        jac = tp / union\n",
    "        \n",
    "        precisions.append(prec)\n",
    "        recalls.append(rec)\n",
    "        f1s.append(f1)\n",
    "        jaccards.append(jac)\n",
    "    print('Precision - ', round(np.mean(precisions), 2))\n",
    "    print('Recall - ', round(np.mean(recalls), 2))\n",
    "    print('F1 - ', round(np.mean(f1s), 2))\n",
    "    print('Jaccard - ', round(np.mean(jaccards), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Токенизация, удаление стоп-слов и нормализация."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Сравним результаты mystem и pymorphy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = punctuation + '«»—…“”*№–'\n",
    "stops = set(stopwords.words('russian'))\n",
    "mystem = Mystem()\n",
    "morph = MorphAnalyzer()\n",
    "\n",
    "def normalize(text, method='mystem'):\n",
    "    \n",
    "    words = [word.strip(punct) for word in text.lower().split()]\n",
    "    if method == 'pymorphy':\n",
    "        words = [morph.parse(word)[0].normal_form for word in words if word and word not in stops]\n",
    "    else:\n",
    "        words = [mystem.lemmatize(word)[0] for word in words if word and word not in stops]\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['content_norm'] = data['content'].apply(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['title_norm'] = data['title'].apply(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.11\n",
      "Recall -  0.22\n",
      "F1 -  0.14\n",
      "Jaccard -  0.08\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], data['content_norm'].apply(lambda x: [x[0] for x in Counter(x).most_common(10)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В исходном варианте (c pymorphy) качество на этом шаге ниже:\n",
    "* Precision -  0.1\n",
    "* Recall -  0.21\n",
    "* F1 -  0.13\n",
    "* Jaccard -  0.08\n",
    "\n",
    "Поэтому в дальнейшем будет использоваться mystem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Добавим в список стоп-слов сокращения от единиц измерений, названий валют и т.п.:\n",
    "Найдем наиболее частотные сокращения, которые анализируются как существительные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Counter()\n",
    "\n",
    "for elem in chain.from_iterable(data['content_norm']):\n",
    "    if len(elem) < 5:\n",
    "        d[elem] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('год', 11167),\n",
       " ('это', 9217),\n",
       " ('свой', 4722),\n",
       " ('один', 3026),\n",
       " ('мочь', 2204),\n",
       " ('наш', 2182),\n",
       " ('сша', 2089),\n",
       " ('дело', 1991),\n",
       " ('мир', 1952),\n",
       " ('тот', 1894),\n",
       " ('быть', 1857),\n",
       " ('этот', 1604),\n",
       " ('день', 1571),\n",
       " ('сила', 1549),\n",
       " ('идти', 1423),\n",
       " ('рф', 1288),\n",
       " ('сам', 1240),\n",
       " ('два', 1206),\n",
       " ('тыс', 1173),\n",
       "  ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#просмотрим соответствующий список (полностью)\nd.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим в стоп-слова:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops2 = stops.union({'тыс', 'млрд', 'нга', 'м', 'т', 'долл', 'км', 'год',\n",
    "                 'бла', 'куб', 'евро', 'см', 'г', 'кг', 'трлн', 'др', 'кв', 'га'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystem = Mystem()\n",
    "\n",
    "def normalize2(text):\n",
    "    \n",
    "    words = [word.strip(punct) for word in text.lower().split()]\n",
    "    nouns = []\n",
    "    for word in words:\n",
    "        if word and word not in stops2:\n",
    "            analysis = mystem.analyze(word)[0].get('analysis')\n",
    "            if analysis and analysis[0]['gr'].split(',')[0] == 'S':\n",
    "                nouns.append(analysis[0]['lex'])\n",
    "                \n",
    "    return nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['content_norm_2'] = data['content'].apply(normalize2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.13\n",
      "Recall -  0.26\n",
      "F1 -  0.17\n",
      "Jaccard -  0.1\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], data['content_norm_2'].apply(lambda x: [x[0] for x in Counter(x).most_common(10)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В исходном варианте качество на этом шаге:\n",
    "* Precision -  0.12\n",
    "* Recall -  0.24\n",
    "* F1 -  0.15\n",
    "* Jaccard -  0.09"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Проанализируем заголовки вместе с основным текстом:\n",
    "Это может помочь, поскольку, по результатам, полученным на семинаре, нахождение ключевых слов только из заголовков показывает результаты не намного хуже, чем нахождение ключевых слов из целых текстов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['norm'] = (data['content'] + data['title']).apply(normalize2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.14\n",
      "Recall -  0.27\n",
      "F1 -  0.17\n",
      "Jaccard -  0.1\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], data['norm'].apply(lambda x: [x[0] for x in Counter(x).most_common(10)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Улучшилось ```precision``` и ```recall``` на 0.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Воспользуемся TfidfVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1,2), min_df=5, lowercase=False, tokenizer=lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=False, max_df=1.0, max_features=None, min_df=5,\n",
       "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=<function <lambda> at 0x7f11dcc36730>, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.fit(data['content_norm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = {i:word for i,word in enumerate(tfidf.get_feature_names())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_vectors = tfidf.transform(data['norm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### топ-10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [[id2word[w] for w in top] for top in texts_vectors.toarray().argsort()[:,:-11:-1]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.14\n",
      "Recall -  0.27\n",
      "F1 -  0.18\n",
      "Jaccard -  0.1\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В исходном варианте качество на этом шаге:\n",
    "* Precision -  0.13\n",
    "* Recall -  0.24\n",
    "* F1 -  0.16\n",
    "* Jaccard -  0.09"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Возьмем топ-5 вместо топ-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [[id2word[w] for w in top] for top in texts_vectors.toarray().argsort()[:,:-6:-1]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.2\n",
      "Recall -  0.2\n",
      "F1 -  0.19\n",
      "Jaccard -  0.12\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выросло ```F1``` и ```precision``` и ```jaccard```, но, к сожалению, уменьшился ```recall``` (в сравнении с топ-10)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Попробуем графы!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_matrix(text, window_size=5):\n",
    "    vocab = set(text)\n",
    "    word2id = {w:i for i, w in enumerate(vocab)}\n",
    "    id2word = {i:w for i, w in enumerate(vocab)}\n",
    "    # преобразуем слова в индексы для удобства\n",
    "    ids = [word2id[word] for word in text]\n",
    "\n",
    "    # создадим матрицу совстречаемости\n",
    "    m = np.zeros((len(vocab), len(vocab)))\n",
    "\n",
    "    # пройдемся окном по всему тексту\n",
    "    for i in range(0, len(ids), window_size):\n",
    "        window = ids[i:i+window_size]\n",
    "        # добавим единичку всем парам слов в этом окне\n",
    "        for j, k in combinations(window, 2):\n",
    "            # чтобы граф был ненаправленный \n",
    "            m[j][k] += 1\n",
    "            m[k][j] += 1\n",
    "    \n",
    "    return m, id2word\n",
    "\n",
    "def some_centrality_measure(text, window_size=5, topn=5):\n",
    "    \n",
    "    matrix, id2word = build_matrix(text, window_size)\n",
    "    G = nx.from_numpy_array(matrix)\n",
    "    # тут можно поставить любую метрику\n",
    "    node2measure = dict(nx.degree(G))\n",
    "    \n",
    "    return [id2word[index] for index,measure in sorted(node2measure.items(), key=lambda x: -x[1])[:topn]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### топ-10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.14\n",
      "Recall -  0.26\n",
      "F1 -  0.17\n",
      "Jaccard -  0.1\n"
     ]
    }
   ],
   "source": [
    "keyword_nx = data['norm'].apply(lambda x: some_centrality_measure(x, 10, 10))\n",
    "evaluate(data['keywords'], keyword_nx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В исходном варианте качество на этом шаге:\n",
    "* Precision -  0.12\n",
    "* Recall -  0.24\n",
    "* F1 -  0.16\n",
    "* Jaccard -  0.09"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Возьмем топ-5 вместо топ-10: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.19\n",
      "Recall -  0.19\n",
      "F1 -  0.18\n",
      "Jaccard -  0.11\n"
     ]
    }
   ],
   "source": [
    "keyword_nx = data['norm'].apply(lambda x: some_centrality_measure(x, 10, 5))\n",
    "evaluate(data['keywords'], keyword_nx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также значения всех метрик, кроме ```recall```, увеличились (в сравнении с топ-10)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
