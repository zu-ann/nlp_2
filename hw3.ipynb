{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import log\n",
    "from lxml import html\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from collections import Counter, defaultdict\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from pymystem3 import Mystem\n",
    "\n",
    "mystem = Mystem()\n",
    "punct = punctuation+'«»—…“”*№–'\n",
    "stops = set(stopwords.words('russian'))\n",
    "\n",
    "def normalize(text):\n",
    "    \n",
    "    words = [word.strip(punct) for word in text.lower().split()]\n",
    "    words = [mystem.lemmatize(word)[0] for word in words if word and word not in stops]\n",
    "    words = [word for word in words if word not in stops]\n",
    "\n",
    "    return words\n",
    "\n",
    "def tokenize(text):\n",
    "    \n",
    "    words = [word.strip(punct) for word in text.lower().split()]\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rt = pd.read_csv('news_texts.csv', error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rt.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rt['content_norm'] = data_rt['content_norm'].apply(str.split)\n",
    "data_rt['tokenized'] = data_rt['content'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rt.to_csv('news_texts.tsv', sep='\\t', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>content_norm</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Канцлер Германии Ангела Меркель в ходе брифинг...</td>\n",
       "      <td>[канцлер, германия, ангел, меркель, ход, брифи...</td>\n",
       "      <td>[канцлер, германии, ангела, меркель, в, ходе, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Российские и белорусские войска успешно заверш...</td>\n",
       "      <td>[российский, белорусский, войско, успешно, зав...</td>\n",
       "      <td>[российские, и, белорусские, войска, успешно, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Дзюба, Шатов и Анюков оказались не нужны «Зени...</td>\n",
       "      <td>[дзюба, шат, анюк, оказаться, нужный, зенит, р...</td>\n",
       "      <td>[дзюба, шатов, и, анюков, оказались, не, нужны...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>В Испанию без фанатов\\nПожалуй, главной пятнич...</td>\n",
       "      <td>[испания, фанат, пожалуй, главный, пятничный, ...</td>\n",
       "      <td>[в, испанию, без, фанатов, пожалуй, главной, п...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Постпред России при ООН Виталий Чуркин, говоря...</td>\n",
       "      <td>[постпред, россия, оон, виталий, чуркин, говор...</td>\n",
       "      <td>[постпред, россии, при, оон, виталий, чуркин, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0  Канцлер Германии Ангела Меркель в ходе брифинг...   \n",
       "1  Российские и белорусские войска успешно заверш...   \n",
       "2  Дзюба, Шатов и Анюков оказались не нужны «Зени...   \n",
       "3  В Испанию без фанатов\\nПожалуй, главной пятнич...   \n",
       "4  Постпред России при ООН Виталий Чуркин, говоря...   \n",
       "\n",
       "                                        content_norm  \\\n",
       "0  [канцлер, германия, ангел, меркель, ход, брифи...   \n",
       "1  [российский, белорусский, войско, успешно, зав...   \n",
       "2  [дзюба, шат, анюк, оказаться, нужный, зенит, р...   \n",
       "3  [испания, фанат, пожалуй, главный, пятничный, ...   \n",
       "4  [постпред, россия, оон, виталий, чуркин, говор...   \n",
       "\n",
       "                                           tokenized  \n",
       "0  [канцлер, германии, ангела, меркель, в, ходе, ...  \n",
       "1  [российские, и, белорусские, войска, успешно, ...  \n",
       "2  [дзюба, шатов, и, анюков, оказались, не, нужны...  \n",
       "3  [в, испанию, без, фанатов, пожалуй, главной, п...  \n",
       "4  [постпред, россии, при, оон, виталий, чуркин, ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_rt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Коллекция состоит из пар предложения (заголоков статей) и метки класса (-1,0,1). -1 не парафраз, 1 - парафраз, 0 - что-то непонятное."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_xml = html.fromstring(open('paraphrases.xml', 'rb').read())\n",
    "texts_1 = []\n",
    "texts_2 = []\n",
    "classes = []\n",
    "\n",
    "for p in corpus_xml.xpath('//paraphrase'):\n",
    "    texts_1.append(p.xpath('./value[@name=\"text_1\"]/text()')[0])\n",
    "    texts_2.append(p.xpath('./value[@name=\"text_2\"]/text()')[0])\n",
    "    classes.append(p.xpath('./value[@name=\"class\"]/text()')[0])\n",
    "    \n",
    "data = pd.DataFrame({'text_1':texts_1, 'text_2':texts_2, 'label':classes})\n",
    "\n",
    "data['text_1_norm'] = data['text_1'].apply(normalize)\n",
    "data['text_2_norm'] = data['text_2'].apply(normalize)\n",
    "data['text_1_tokenized'] = data['text_1'].apply(tokenize)\n",
    "data['text_2_tokenized'] = data['text_2'].apply(tokenize)\n",
    "\n",
    "data.to_csv('paraphrases.tsv', sep='\\t', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rt = pd.read_csv('news_texts.tsv', sep='\\t')\n",
    "data = pd.read_csv('paraphrases.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = CountVectorizer(min_df=3, max_df=0.4, max_features=1000, lowercase=False, tokenizer=lambda x: x)\n",
    "X_count = count.fit_transform(data_rt['content_norm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(min_df=3, max_df=0.4, max_features=1000, lowercase=False, tokenizer=lambda x: x)\n",
    "X_tfidf = tfidf.fit_transform(data_rt['content_norm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, model, dim, n_documents=None, inv_idx=None):\n",
    "#     text = text.split()\n",
    "    words = Counter(text)\n",
    "    total = len(text)\n",
    "    vectors = np.zeros((len(words), dim))\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        try:\n",
    "            v = model.wv[word]\n",
    "            if inv_idx:\n",
    "                vectors[i] = v * (words[word] / total) * log(n_documents / inv_idx[word])\n",
    "            else:\n",
    "                vectors[i] = v\n",
    "        except (KeyError, ValueError):\n",
    "            continue\n",
    "    \n",
    "    if vectors.any():\n",
    "        vector = np.average(vectors, axis=0)\n",
    "    else:\n",
    "        vector = np.zeros((dim))\n",
    "    \n",
    "    return vector       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_texts(data, model, dim, inv_idx=None):\n",
    "    n_documents = len(data)\n",
    "    X_text = np.zeros((n_documents, dim))\n",
    "                \n",
    "    for i, text in enumerate(data):\n",
    "        X_text[i] = get_embedding(text, model, dim, n_documents * 2, inv_idx)\n",
    "    \n",
    "    return X_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inv_idx(data):\n",
    "    inverted_index = defaultdict(list)\n",
    "    for i, doc in enumerate(data):\n",
    "        for word in doc: #.split():\n",
    "            inverted_index[word].append(i)\n",
    "    \n",
    "    inv_idx = {word:len(inverted_index[word]) for word in inverted_index}\n",
    "    return inv_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(v1, v2):\n",
    "    v1_norm = gensim.matutils.unitvec(np.array(v1))\n",
    "    v2_norm = gensim.matutils.unitvec(np.array(v2))\n",
    "    return np.dot(v1_norm, v2_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity(model, data_1, data_2, dim, vect=None, embeddings_needed=False,\n",
    "                                           weighted_tfidf=False, tokenized=False):\n",
    "    if embeddings_needed:\n",
    "        \n",
    "        if weighted_tfidf:\n",
    "            if tokenized:\n",
    "                X_text_1 = transform_texts(data_1.values, model, dim, inv_idx_tokenized)\n",
    "                X_text_2 = transform_texts(data_2.values, model, dim, inv_idx_tokenized)\n",
    "            else:\n",
    "                X_text_1 = transform_texts(data_1.values, model, dim, inv_idx)\n",
    "                X_text_2 = transform_texts(data_2.values, model, dim, inv_idx)\n",
    "        else:    \n",
    "            X_text_1 = transform_texts(data_1.values, model, dim)\n",
    "            X_text_2 = transform_texts(data_2.values, model, dim)\n",
    "    else:\n",
    "        \n",
    "        X_text_1 = model.transform(vect.transform(data_1))\n",
    "        X_text_2 = model.transform(vect.transform(data_2))\n",
    "        \n",
    "    sim = [similarity(v1, v2) for v1, v2 in zip(X_text_1, X_text_2)]\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_count = NMF(dim)\n",
    "nmf_count.fit(X_count)\n",
    "res_nmf_count = get_similarity(nmf_count, data['text_1_norm'], data['text_2_norm'], dim, vect=count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_tfidf = NMF(dim)\n",
    "nmf_tfidf.fit(X_tfidf)\n",
    "res_nmf_tfidf = get_similarity(nmf_tfidf, data['text_1_norm'], data['text_2_norm'], dim, vect=tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_count = TruncatedSVD(dim)\n",
    "svd_count.fit(X_count)\n",
    "res_svd_count = get_similarity(svd_count, data['text_1_norm'], data['text_2_norm'], dim, vect=count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_tfidf = TruncatedSVD(dim)\n",
    "svd_tfidf.fit(X_tfidf)\n",
    "res_svd_tfidf = get_similarity(svd_tfidf, data['text_1_norm'], data['text_2_norm'], dim, vect=tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_idx = get_inv_idx(np.concatenate([data['text_1_norm'], data['text_2_norm']], axis=0))\n",
    "inv_idx_tokenized = get_inv_idx(np.concatenate([data['text_1_tokenized'], data['text_1_tokenized']], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_text = gensim.models.FastText(data_rt['tokenized'], size=dim, min_n=4, max_n=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_ft = get_similarity(fast_text, data['text_1_tokenized'], data['text_2_tokenized'], dim, \n",
    "                        embeddings_needed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_ft_tfidf = get_similarity(fast_text, data['text_1_tokenized'], data['text_2_tokenized'], dim, \n",
    "                        embeddings_needed=True, weighted_tfidf=True, tokenized=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_text_norm = gensim.models.FastText(data_rt['content_norm'],\n",
    "                                        size=50, min_n=4, max_n=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_ft_norm = get_similarity(fast_text_norm, data['text_1_norm'], data['text_2_norm'], dim, \n",
    "                        embeddings_needed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_ft_norm_tfidf = get_similarity(fast_text_norm, data['text_1_norm'], data['text_2_norm'], dim, \n",
    "                        embeddings_needed=True, weighted_tfidf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = gensim.models.Word2Vec(data_rt['content_norm'], size=dim, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_w2v = get_similarity(w2v, data['text_1_norm'], data['text_2_norm'], dim, \n",
    "                        embeddings_needed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_w2v_tfidf = get_similarity(w2v, data['text_1_norm'], data['text_2_norm'], dim, \n",
    "                        embeddings_needed=True, weighted_tfidf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ft</th>\n",
       "      <th>ft_norm</th>\n",
       "      <th>ft_norm_tfidf</th>\n",
       "      <th>ft_tfidf</th>\n",
       "      <th>nmf_count</th>\n",
       "      <th>nmf_tfidf</th>\n",
       "      <th>svd_count</th>\n",
       "      <th>svd_tfidf</th>\n",
       "      <th>w2v</th>\n",
       "      <th>w2v_tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.994350</td>\n",
       "      <td>0.992412</td>\n",
       "      <td>0.991473</td>\n",
       "      <td>0.997758</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.998637</td>\n",
       "      <td>0.996784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.988949</td>\n",
       "      <td>0.986961</td>\n",
       "      <td>0.997860</td>\n",
       "      <td>0.997450</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.997773</td>\n",
       "      <td>0.999353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.986031</td>\n",
       "      <td>0.996695</td>\n",
       "      <td>0.997308</td>\n",
       "      <td>0.996961</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999345</td>\n",
       "      <td>0.998966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.992464</td>\n",
       "      <td>0.969356</td>\n",
       "      <td>0.985210</td>\n",
       "      <td>0.996097</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.996024</td>\n",
       "      <td>0.995628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.967080</td>\n",
       "      <td>0.967738</td>\n",
       "      <td>0.991649</td>\n",
       "      <td>0.995313</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.995571</td>\n",
       "      <td>0.997886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ft   ft_norm  ft_norm_tfidf  ft_tfidf  nmf_count  nmf_tfidf  \\\n",
       "0  0.994350  0.992412       0.991473  0.997758        0.0        0.0   \n",
       "1  0.988949  0.986961       0.997860  0.997450        0.0        0.0   \n",
       "2  0.986031  0.996695       0.997308  0.996961        0.0        0.0   \n",
       "3  0.992464  0.969356       0.985210  0.996097        0.0        0.0   \n",
       "4  0.967080  0.967738       0.991649  0.995313        0.0        0.0   \n",
       "\n",
       "   svd_count  svd_tfidf       w2v  w2v_tfidf  \n",
       "0        0.0        0.0  0.998637   0.996784  \n",
       "1        0.0        0.0  0.997773   0.999353  \n",
       "2        0.0        0.0  0.999345   0.998966  \n",
       "3        0.0        0.0  0.996024   0.995628  \n",
       "4        0.0        0.0  0.995571   0.997886  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data = pd.DataFrame({'nmf_count': res_nmf_count, 'nmf_tfidf': res_nmf_tfidf,\n",
    "                           'svd_count': res_svd_count, 'svd_tfidf': res_svd_tfidf,\n",
    "                           'ft': res_ft, 'ft_tfidf': res_ft_tfidf,\n",
    "                           'ft_norm': res_ft_norm, 'ft_norm_tfidf': res_ft_norm_tfidf,\n",
    "                           'w2v': res_w2v, 'w2v_tfidf': res_w2v_tfidf})\n",
    "final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43972562264702686"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression(class_weight='balanced')\n",
    "cross_val_score(logreg, final_data, data['label'], cv=5, scoring='f1_micro', n_jobs=-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49050360555015005"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest = RandomForestClassifier(n_estimators=500, class_weight='balanced')\n",
    "cross_val_score(random_forest, final_data, data['label'], cv=5, scoring='f1_micro', n_jobs=-1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Увеличим размерность в ```w2v``` до 100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = gensim.models.Word2Vec(data_rt['content_norm'], size=100, sg=1)\n",
    "res_w2v = get_similarity(w2v, data['text_1_norm'], data['text_2_norm'], dim, \n",
    "                        embeddings_needed=True)\n",
    "res_w2v_tfidf = get_similarity(w2v, data['text_1_norm'], data['text_2_norm'], dim, \n",
    "                        embeddings_needed=True, weighted_tfidf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.update(pd.DataFrame({'w2v': res_w2v, 'w2v_tfidf': res_w2v_tfidf}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4361299709454693"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression(class_weight='balanced')\n",
    "cross_val_score(logreg, final_data, data['label'], cv=5, scoring='f1_micro', n_jobs=-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4700255770143123"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest = RandomForestClassifier(n_estimators=500, class_weight='balanced')\n",
    "cross_val_score(random_forest, final_data, data['label'], cv=5, scoring='f1_micro', n_jobs=-1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Немного падает качество! Вернем прошлые значения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.update(pd.DataFrame({'w2v': res_w2v, 'w2v_tfidf': res_w2v_tfidf}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Увеличим размерность в ```svd``` до 100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 100\n",
    "\n",
    "svd_count = TruncatedSVD(dim)\n",
    "svd_count.fit(X_count)\n",
    "res_svd_count2 = get_similarity(svd_count, data['text_1_norm'], data['text_2_norm'], dim, vect=count)\n",
    "\n",
    "svd_tfidf = TruncatedSVD(dim)\n",
    "svd_tfidf.fit(X_tfidf)\n",
    "res_svd_tfidf2 = get_similarity(svd_tfidf, data['text_1_norm'], data['text_2_norm'], dim, vect=tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.update(pd.DataFrame({'svd_count': res_svd_count2, 'svd_tfidf': res_svd_tfidf2}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43986393549834435"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(logreg, final_data, data['label'], cv=5, scoring='f1_micro', n_jobs=-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48939432744334643"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(random_forest, final_data, data['label'], cv=5, scoring='f1_micro', n_jobs=-1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество немного улучшается! Увеличим размерность в ```nmf``` до 100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 100\n",
    "\n",
    "nmf_count = NMF(dim)\n",
    "nmf_count.fit(X_count)\n",
    "res_nmf_count2 = get_similarity(nmf_count, data['text_1_norm'], data['text_2_norm'], dim, vect=count)\n",
    "\n",
    "nmf_tfidf = NMF(dim)\n",
    "nmf_tfidf.fit(X_tfidf)\n",
    "res_nmf_tfidf2 = get_similarity(nmf_tfidf, data['text_1_norm'], data['text_2_norm'], dim, vect=tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.update(pd.DataFrame({'nmf_count': res_nmf_count2, 'nmf_tfidf': res_nmf_tfidf2}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43986393549834435"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(logreg, final_data, data['label'], cv=5, scoring='f1_micro', n_jobs=-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49063885555332465"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(random_forest, final_data, data['label'], cv=5, scoring='f1_micro', n_jobs=-1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У ```random forest``` качество ещё немного улучшается! Уменьшим значение ```min_n``` до 2 в ```fast_text```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_text = gensim.models.FastText(data_rt['tokenized'], size=dim, min_n=2, max_n=8)\n",
    "res_ft2 = get_similarity(fast_text, data['text_1_tokenized'], data['text_2_tokenized'], dim, \n",
    "                        embeddings_needed=True)\n",
    "res_ft_tfidf2 = get_similarity(fast_text, data['text_1_tokenized'], data['text_2_tokenized'], dim, \n",
    "                        embeddings_needed=True, weighted_tfidf=True, tokenized=True)\n",
    "fast_text_norm = gensim.models.FastText(data_rt['content_norm'],\n",
    "                                        size=50, min_n=4, max_n=8)\n",
    "res_ft_norm2 = get_similarity(fast_text_norm, data['text_1_norm'], data['text_2_norm'], dim, \n",
    "                        embeddings_needed=True)\n",
    "res_ft_norm_tfidf2 = get_similarity(fast_text_norm, data['text_1_norm'], data['text_2_norm'], dim, \n",
    "                        embeddings_needed=True, weighted_tfidf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.update(pd.DataFrame({'ft': res_ft2, 'ft_tfidf': res_ft_tfidf2,\n",
    "                                'ft_norm': res_ft_norm2, 'ft_norm_tfidf': res_ft_norm_tfidf2}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4332233949612233"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(logreg, final_data, data['label'], cv=5, scoring='f1_micro', n_jobs=-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47819703103777744"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(random_forest, final_data, data['label'], cv=5, scoring='f1_micro', n_jobs=-1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество не улучшается!  \n",
    "  \n",
    "В итоге помогло увеличение размерности ```dim = 100``` для ```svd``` и ```nmf```."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
